# /Users/lina/Desktop/ë°ì´í„°/collect_online_buzz_timeseries.py

import os
import json
import time
import math
import datetime as dt
import pandas as pd
import requests
from tqdm import tqdm
from dotenv import load_dotenv
from collections import defaultdict

# =======================
# 1) .env ë¡œë“œ (NAVER í‚¤)
# =======================
load_dotenv()
CID  = os.getenv("NAVER_CLIENT_ID")
CSEC = os.getenv("NAVER_CLIENT_SECRET")
if not CID or not CSEC:
    raise RuntimeError("âŒ NAVER_CLIENT_ID / NAVER_CLIENT_SECRETì´ .envì— ì—†ìŠµë‹ˆë‹¤.")

# =======================
# 2) ê²½ë¡œ/ì…ë ¥ íŒŒì¼
# =======================
DATA_DIR = "/Users/lina/Desktop/ë°ì´í„°"
OUTPUT_PATH = os.path.join(DATA_DIR, "Online_Buzz.csv")

BOX_FILES = ["koreanfilms.csv"]  # âœ… ìƒˆ CSV í•˜ë‚˜ë§Œ ì‚¬ìš©

# ë„¤ì´ë²„ ë°ì´í„°ë© ì‚¬ì–‘
MAX_GROUPS_PER_CALL = 5
DATA_LBOUND = dt.date(2016, 1, 1)
TODAY = dt.date.today()

# =======================
# 3) CSV ë¡œë“œ (ì¡°ì¸ ì—†ìŒ)
# =======================
def load_films() -> pd.DataFrame:
    path = os.path.join(DATA_DIR, BOX_FILES[0])
    if not os.path.exists(path):
        raise FileNotFoundError(path)

    df = pd.read_csv(path)

    # openDt: "YYYYMMDD" í˜•ì‹ â†’ datetime
    # (ì´ë¯¸ 2023 ì´í›„ í•„í„°ëœ íŒŒì¼ì´ì§€ë§Œ, í•œ ë²ˆ ë” ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
    df["open_dt"] = pd.to_datetime(df["openDt"].astype(str), format="%Y%m%d", errors="coerce")
    df = df.dropna(subset=["open_dt"])
    df = df[df["open_dt"] >= pd.Timestamp("2023-01-01")].copy()

    # movie_nm = title
    df["movie_nm"] = df["title"].astype(str)

    # ìµœì†Œ ì»¬ëŸ¼ë§Œ ì‚¬ìš©
    df = df[["movieCd", "movie_nm", "open_dt"]].reset_index(drop=True)

    print(f"ğŸ¬ ëŒ€ìƒ ì˜í™” ìˆ˜(ê°œë´‰ì¼ ì¡´ì¬): {len(df)}")
    print("ğŸ” ìƒ˜í”Œ 5í–‰:")
    print(df.head())

    return df

def clean_title(t: str) -> str:
    # ê²€ìƒ‰ì–´ë¡œ ì“¸ ì œëª© ì •ë¦¬ (íŠ¹ìˆ˜ë¬¸ì ì¡°ê¸ˆë§Œ ì •ë¦¬)
    return str(t).replace("/", " ").replace(":", " ").strip()

# =======================
# 4) ë„¤ì´ë²„ ë°ì´í„°ë© API - ë°°ì¹˜ í˜¸ì¶œ + ë°±ì˜¤í”„
# =======================
def datalab_search_batch(keyword_groups, start, end, max_retries=5):
    """
    keyword_groups: [{"groupName": "ì˜í™”ëª…", "keywords": ["ì˜í™”ëª…"]}, ...]  (len <= 5)
    start, end: "YYYY-MM-DD"
    """
    url = "https://openapi.naver.com/v1/datalab/search"
    headers = {
        "X-Naver-Client-Id": CID,
        "X-Naver-Client-Secret": CSEC,
        "Content-Type": "application/json; charset=UTF-8",
    }
    payload = {
        "startDate": start,
        "endDate": end,
        "timeUnit": "date",
        "keywordGroups": keyword_groups,
        "device": "", "gender": "", "ages": [],
    }

    backoff = 1.0
    for attempt in range(max_retries + 1):
        resp = requests.post(url, headers=headers, data=json.dumps(payload), timeout=30)
        if resp.status_code == 200:
            return resp.json()
        if resp.status_code == 429:
            ra = resp.headers.get("Retry-After")
            wait_sec = float(ra) if ra else backoff
            print(f"â³ 429: {wait_sec:.1f}s ëŒ€ê¸° í›„ ì¬ì‹œë„ (ì‹œë„ {attempt+1}/{max_retries}) | ê¸°ê°„={start}~{end}")
            time.sleep(wait_sec)
            backoff = min(backoff * 2, 30)
            continue

        # ê·¸ ì™¸ ì—ëŸ¬ â†’ ì¬ì‹œë„ or ì‹¤íŒ¨
        try:
            resp.raise_for_status()
        except Exception as e:
            if attempt >= max_retries:
                raise
            time.sleep(backoff)
            backoff = min(backoff * 2, 30)

    resp.raise_for_status()

def extract_series_list(js):
    """
    ë°˜í™˜: [ {period: ratio, ...}, ... ]
    keywordGroups ìˆœì„œì™€ ë™ì¼
    """
    results = js.get("results", [])
    out = []
    for res in results:
        data = res.get("data", [])
        out.append({row["period"]: int(round(float(row["ratio"]))) for row in data})
    return out

# =======================
# 5) ë©”ì¸ ë¡œì§
# =======================
def main():
    films = load_films()

    # âœ… Resume ê¸°ëŠ¥: ì´ë¯¸ ìˆ˜ì§‘ëœ buzz_idëŠ” ìŠ¤í‚µ
    existing_ids = set()
    if os.path.exists(OUTPUT_PATH):
        try:
            prev = pd.read_csv(OUTPUT_PATH)
            if {"buzz_id", "movieCd", "movie_nm", "buzz_date", "search_buzz_vol"}.issubset(prev.columns):
                existing_ids = set(prev["buzz_id"].astype(str).tolist())
                print(f"â™»ï¸ ê¸°ì¡´ ìˆ˜ì§‘ ê±´ìˆ˜: {len(existing_ids)} (buzz_id)")
        except Exception as e:
            print(f"ê¸°ì¡´ ê²°ê³¼ ì½ê¸° ì‹¤íŒ¨(ë¬´ì‹œí•˜ê³  ìƒˆë¡œ ì‘ì„±): {e}")

    # ì˜í™”ë³„ ê¸°ê°„ ê³„ì‚° & (start,end) ë²„í‚·ìœ¼ë¡œ ë¬¶ê¸°
    buckets = defaultdict(list)  # {(start,end): [(movieCd, movie_nm), ...]}

    for r in films.itertuples(index=False):
        odt = r.open_dt.date() if isinstance(r.open_dt, pd.Timestamp) else r.open_dt

        # ê°œë´‰ì¼ ê¸°ì¤€ D0 ~ min(D+365, ì˜¤ëŠ˜)
        start_date = max(odt, DATA_LBOUND)
        end_date   = min(odt + dt.timedelta(days=365), TODAY)
        if end_date < start_date:
            # ì•„ì§ ê°œë´‰ ì „ì´ê±°ë‚˜ ì´ìƒí•œ ë°ì´í„°
            # print(f"â­ï¸ ì•„ì§ ê°œë´‰ ì „: {r.movie_nm} ({r.movieCd}) start={start_date}, end={end_date}")
            continue

        start = start_date.strftime("%Y-%m-%d")
        end   = end_date.strftime("%Y-%m-%d")
        buckets[(start, end)].append((str(r.movieCd), clean_title(r.movie_nm)))

    # ì „ì²´ ë°°ì¹˜ ìˆ˜ ê³„ì‚° (progress barìš©)
    total_batches = sum(math.ceil(len(v) / MAX_GROUPS_PER_CALL) for v in buckets.values())
    pbar = tqdm(total=total_batches, desc="Batch Calls")

    rows = []

    # ë²„í‚·ë³„ & 5ê°œì”© ì˜ë¼ì„œ ë°°ì¹˜ í˜¸ì¶œ
    for (start, end), items in buckets.items():
        for i in range(0, len(items), MAX_GROUPS_PER_CALL):
            chunk = items[i:i + MAX_GROUPS_PER_CALL]
            groups = [{"groupName": t, "keywords": [t]} for (_, t) in chunk]

            try:
                js = datalab_search_batch(groups, start, end, max_retries=6)
                series_list = extract_series_list(js)

                for (movieCd, title), series in zip(chunk, series_list):
                    for d, val in series.items():
                        buzz_id = f"{movieCd}_{d}"
                        if buzz_id in existing_ids:
                            continue
                        rows.append({
                            "buzz_id": buzz_id,          # PK
                            "movieCd": movieCd,          # ì½”ë“œ
                            "movie_nm": title,           # ì œëª© (= title)
                            "buzz_date": d,              # ë‚ ì§œ (ìˆìœ¼ë©´ ë¶„ì„ í¸í•¨)
                            "search_buzz_vol": val       # 0~100
                        })
                time.sleep(0.8)  # ë°°ì¹˜ë¼ ì½œ ìˆ˜ ì ìœ¼ë‹ˆ ì—¬ìœ ë§Œ ì¤Œ

            except requests.HTTPError as e:
                print(f"âš ï¸ ë°°ì¹˜ ì˜¤ë¥˜: {e} | ê¸°ê°„={start}~{end} | ìƒ˜í”Œ={chunk[0][1]}")
                time.sleep(2.0)
            except Exception as e:
                print(f"âš ï¸ ì˜ˆì™¸: {e} | ê¸°ê°„={start}~{end}")
                time.sleep(1.0)
            finally:
                pbar.update(1)

    pbar.close()

    # =======================
    # 6) ì €ì¥
    # =======================
    if rows:
        new_df = pd.DataFrame(rows)
        new_df = new_df.drop_duplicates(subset=["buzz_id"])

        # ìµœì¢… ì»¬ëŸ¼ ìˆœì„œ: buzz_id, movieCd, movie_nm, search_buzz_vol (+ buzz_date ë³´ë„ˆìŠ¤)
        cols = ["buzz_id", "movieCd", "movie_nm", "buzz_date", "search_buzz_vol"]
        new_df = new_df[cols]

        # ê¸°ì¡´ íŒŒì¼ ìˆìœ¼ë©´ í•©ì¹˜ê¸°
        if os.path.exists(OUTPUT_PATH):
            try:
                prev = pd.read_csv(OUTPUT_PATH)
                df = pd.concat([prev, new_df], ignore_index=True)
                df = df.drop_duplicates(subset=["buzz_id"])
            except Exception:
                df = new_df.copy()
        else:
            df = new_df.copy()

        df = df.sort_values(["movieCd", "buzz_date"])
        df.to_csv(OUTPUT_PATH, index=False, encoding="utf-8-sig")
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {OUTPUT_PATH}")
        print(df.head())
        print(df.tail())
    else:
        print("âŒ ìƒˆë¡œ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
